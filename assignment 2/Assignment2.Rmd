---
title: "Regression and Classification: A Comprehensive Analysis"
author: "Gabriele Durante"
output:
  html_document:
  df_print: paged
  pdf_document: default
university: "University of Oslo"
course: "STK-IN4300: Statistical Learning Methods in Data Science"
---

In this report, we aim to explore various statistical learning methodologies applied to regression and classification problems.

In the first part of the report, we examine linear regression models applied to aquatic toxicity data. This involves both traditional linear effects and alternative representations using dummy encoding, providing insight into how these transformations impact model fit and predictive accuracy. We apply techniques such as backward elimination and forward selection to perform variable selection, comparing criteria like AIC and BIC. Moreover, we introduce regularization techniques, specifically ridge regression, to address potential overfitting, followed by a discussion on parameter optimization via cross-validation and bootstrap methods. To extend our analysis to non-linear modeling, we apply generalized additive models (GAMs) with smoothing splines to capture more complex relationships within the data. Regression trees are explored, where we employ cost-complexity pruning to optimize tree size, emphasizing the balance between model complexity and prediction accuracy.

In the second part, we shift to a classification problem using the Pima Indians Diabetes dataset. Here, we experiment with k-nearest neighbors (k-NN), generalized additive models, classification trees, bagged trees, random forests, and neural networks. Each method is rigorously evaluated, using cross-validation techniques to assess generalization error and performance trade-offs. The report concludes with a comparative analysis of these models, offering insights into the advantages and limitations of different statistical learning approaches within the context of high-dimensional data.

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,             # Suppress warnings in the document
  message = FALSE,             # Suppress messages in the document
  fig.align = "center",        # Center align figures
  fig.width = 6,               # Set default width of figures to fit two-column layout
  fig.height = 4,              # Set default height of figures
  fig.pos = "H",               # Force figure placement exactly where they are called
  cache = TRUE                 # Enable caching
)
```

```{r, echo=FALSE, results= 'hide'}
data <- read.csv("/Users/gabrieledurante/Documents/UiO/STK-IN4300 - Statistical Learning/assignment 2/qsar_aquatic_toxicity.csv",
                 sep=";", header = FALSE)
colnames(data) <- c("TPSA", "SAacc", "H050",
                    "MLOGP", "RDCHI", "GATS1p",
                    "nN", "C040", "LC50")
```

# Regression Model Comparison for Predicting Aquatic Toxicity

In this study, we investigate the acute aquatic toxicity of various organic molecules, as quantified by the lethal concentration (LC50) that induces mortality in 50% of the planktonic crustacean Daphnia magna over a 48-hour exposure period. The objective is to develop predictive models that leverage molecular descriptors to ascertain the toxicity levels of these compounds. To this end, we utilize a dataset comprising 546 observations, which is sourced from the UCI Machine Learning Repository. The dataset encompasses eight key molecular descriptors identified as significant predictors of LC50:

-   TPSA: Topological Polar Surface Area, calculated via a contribution method considering the presence of nitrogen, oxygen, potassium, and sulfur.
-   SAacc: Van der Waals Surface Area (VSA) of hydrogen bond acceptor atoms.
-   H050: The count of hydrogen atoms bonded to heteroatoms.
-   MLOGP: A measure of lipophilicity, serving as a critical determinant of narcosis.
-   RDCHI: A topological index encapsulating information related to molecular size and branching.
-   GATS1p: An indicator of molecular polarizability.
-   nN: The total number of nitrogen atoms within the molecule.
-   C040: The count of specific carbon atom types, including those found in esters, carboxylic acids, thioesters, carbamic acids, and nitriles.

```{r Summary}
summary(data)
colSums(is.na(data))
```

With a complete dataset comprising 546 observations, we can ensure that our modeling efforts are not hindered by data gaps. This completeness facilitates a more accurate assessment of the relationships between the molecular descriptors and LC50, allowing for a more confident interpretation of the findings and their implications in predicting aquatic toxicity.

## Linear and Dummy Encoding Approach

To evaluate the predictive models effectively, we partition the dataset into training and testing subsets. A random seed is set using `set.seed(2024)` to ensure reproducibility. The training set is constructed by sampling approximately two-thirds of the total observations with the command `index <- sample(1:nrow(data), size = round(2/3 * nrow(data)))`. 

- Training Data: `trainData <- data[index, ]` comprises the selected observations for model fitting.
- Testing Data: `testData <- data[-index, ]` contains the remaining observations for model evaluation.

```{r Splitting, echo=TRUE}
# build the dataset for the training
set.seed(2024)
index <- sample(1:nrow(data), size = round(2/3 * nrow(data)))
trainData <- data[index, ]
testData <- data[-index, ]
```
### Linear Model
First we fitted a linear regression model to predict LC50 using molecular descriptors from the training dataset. The model's performance is summarized as follows:
```{r Linear Model, echo=TRUE}
set.seed(2024)
### Modeling Count Variables Directly as Linear Effects
model1 <- lm(LC50 ~., data = trainData)

pred_model1_train <- predict(model1, newdata = trainData)
pred_model1_test <- predict(model1, newdata = testData)

error_train <- mean((pred_model1_train - trainData$LC50)^2)
error_test <- mean((pred_model1_test - testData$LC50)^2)
```

```{r}
cat("Training Error (Linear):", error_train, "\n")
cat("Test Error (Linear):", error_test, "\n")
```

```{r summary Model 1}
summary(model1)
```
The R-squared value of 0.472 indicates that while the linear model captures some of the variability in the data, it leaves a considerable amount unexplained. Highly significant predictors, such as TPSA (p < 2e-16) and MLOGP (p < 3.83e-10), reveal strong relationships with LC50, indicating that increased topological polar surface area and greater lipophilicity correlate with higher toxicity levels. 

### Dummy Linear Model

After, model dummy encoding was applied to the count variables nN, C040, and H050 in both the training and test datasets to transform these continuous variables into binary indicators. Specifically, values greater than zero were recoded to 1, while values of zero were recoded to 0. This approach allows the linear model to assess the presence or absence of these variables as factors influencing acute aquatic toxicity. Predictions were generated for both the training and test datasets, enabling the calculation of the training error and test error.

```{r Dummy Linear model}
### Dummy Encoding for Count Variables
train_dummy <- trainData
test_dummy <- testData

# dummy encoding
train_dummy$nN <- ifelse(train_dummy$nN > 0, 1, 0)
test_dummy$nN <- ifelse(test_dummy$nN > 0, 1, 0)
test_dummy$C040 <- ifelse(test_dummy$C040 > 0, 1, 0)
train_dummy$C040 <- ifelse(train_dummy$C040 > 0, 1, 0)
test_dummy$H050 <- ifelse(test_dummy$H050 > 0, 1, 0)
train_dummy$H050 <- ifelse(train_dummy$H050 > 0, 1, 0)

model2 <- lm(train_dummy$LC50 ~ ., data = train_dummy)

pred_model2_train <- predict(model2, newdata = train_dummy)
pred_model2_test <- predict(model2, newdata = test_dummy)

error_train_dummy <- mean((pred_model2_train - train_dummy$LC50)^2)
error_test_dummy <- mean((pred_model2_test - test_dummy$LC50)^2)
```

```{r}
cat("Training Error (Dummy):", error_train_dummy, "\n")
cat("Test Error (Dummy):", error_test_dummy, "\n")
```

```{r}
summary(model2)
```
The training error for the dummy-encoded model is 1.424, while the test error is 1.617. Compared to the previous linear model, which yielded a training error of 1.377 and a test error of 1.557, this model exhibits slightly higher error rates on both the training and test datasets. This increase suggests that the dummy encoding may not have improved the model's predictive performance. This could indicate a potential loss of information due to the transformation of continuous variables into binary indicators, which could limit the model's ability to capture the nuances of the underlying relationships between predictors and the response variable.

The overall model performance is reflected in the Multiple R-squared value of 0.454, while the adjusted R-squared of 0.4417 accounts for the number of predictors in the model. The F-statistic (36.9) with a p-value < 2.2e-16 indicates that the model is statistically significant.

## Empirical Error Distribution Analysis in Regression Models

The function `perform_analysis` is defined to conduct repeated evaluations of the two linear models defined before on the same dataset and returns the values of error test and train. The analysis is repeated 200 times using the `replicate` function, enabling the computation of average test errors for both models. The results are then visualized using a density plot, illustrating the empirical distribution of test errors for each model.

```{r 200 times, echo=TRUE}
library(ggplot2)

perform_analysis <- function(data) {
  index <- sample(1:nrow(data), size = round(2/3 * nrow(data)))
  trainData <- data[index, ]
  testData <- data[-index, ]

  # model 1
  model1 <- lm(LC50 ~., data = trainData)
  pred_model1_train_200 <- predict(model1, newdata = trainData)
  pred_model1_test_200 <- predict(model1, newdata = testData)
  error_train_200_lm <- mean((pred_model1_train_200 - trainData$LC50)^2)
  error_test_200_lm <- mean((pred_model1_test_200 - testData$LC50)^2)

  # model 2
  train_dummy <- trainData
  test_dummy <- testData
  
  # dummy encoding
  train_dummy$nN <- ifelse(train_dummy$nN > 0, 1, 0)
  test_dummy$nN <- ifelse(test_dummy$nN > 0, 1, 0)
  test_dummy$C040 <- ifelse(test_dummy$C040 > 0, 1, 0)
  train_dummy$C040 <- ifelse(train_dummy$C040 > 0, 1, 0)
  test_dummy$H050 <- ifelse(test_dummy$H050 > 0, 1, 0)
  train_dummy$H050 <- ifelse(train_dummy$H050 > 0, 1, 0)
  
  model2 <- lm(train_dummy$LC50 ~ ., data = train_dummy)
  
  pred_model2_train <- predict(model2, newdata = train_dummy)
  pred_model2_test <- predict(model2, newdata = test_dummy)
  
  error_train_200_dummy <- mean((pred_model2_train - train_dummy$LC50)^2)
  error_test_200_dummy <- mean((pred_model2_test - test_dummy$LC50)^2)

  return(c(error_test_200_lm, error_test_200_dummy))
}
```

```{r, echo=FALSE, results='asis'}
num_repeats <- 200
results <- replicate(num_repeats, perform_analysis(data))

avg_errors <- colMeans(results)

results_df <- data.frame(
  Error = c(results[1, ], results[2, ]),
  Model = rep(c("Linear Effects", "Dummy Encoding"), each = num_repeats)
)

# Plot
mean_linear <- mean(results_df$Error[results_df$Model == "Linear Effects"])
mean_dummy <- mean(results_df$Error[results_df$Model == "Dummy Encoding"])

ggplot(results_df, aes(x = Error, fill = Model)) +
  geom_density(alpha = 0.5) +
  geom_vline(aes(xintercept = mean_linear, color = "Linear Model Mean"),
             linetype = "dashed", size = 0.5) +
  geom_vline(aes(xintercept = mean_dummy, color = "Dummy Model Mean"),
             linetype = "dashed", size = 0.5) +
  labs(title = "Empirical Distribution of Test Errors",
       x = "Test Error (MSE)",
       y = "Density") +
  scale_color_manual(values = c("Linear Model Mean" = "blue",
                                "Dummy Model Mean" = "red")) +
  theme_light() +
  theme(panel.grid = element_blank(),
          plot.title = element_text(hjust = 0.5),
        text = element_text(size = 10))
```
The graph presents a density plot comparing the test error distributions for the two models. The aim should be illustrate the performance of the models on unseen data and their reliability, focusing on the variability and central tendency of their respective errors. The Linear Model exhibits a narrower distribution centered around a lower MSE compared to the Dummy Model, whose distribution appears wider and shifted slightly to the right, indicating higer average test error. Basically, the tigter distribution and lower mean MSE of the Linear Model suggest that it generalize better to the test set compared to the dummy model.

## Variable Selection Methods in Linear Regression

```{r Backward Elimination}
library(MASS)
library(leaps)

# Backward Elimination
full_model <- lm(LC50 ~ ., data = trainData)
backward_aic <- stepAIC(full_model, direction = "backward",
                        k = 2, trace = FALSE)
summary(backward_aic)
backward_bic <- stepAIC(full_model, direction = "backward",
                        k = log(nrow(trainData)), trace = FALSE)
summary(backward_bic)
```

```{r Forward Selection}
# Forward Selection
null_model <- lm(LC50 ~ 1, data = trainData)
forward_aic <- stepAIC(null_model, direction = "forward",
                       scope = formula(full_model), k = 2, trace = FALSE)
summary(forward_aic)
forward_bic <- stepAIC(null_model, direction = "forward",
                       scope = formula(full_model), k = log(nrow(trainData)), trace = FALSE)
summary(forward_bic)
```

```{r Compare Models}
# Compare Models
cat("Backward Elimination (AIC) Model:\n")
print(backward_aic$call)
cat("\nBackward Elimination (BIC) Model:\n")
print(backward_bic$call)
cat("\nForward Selection (AIC) Model:\n")
print(forward_aic$call)
cat("\nForward Selection (BIC) Model:\n")
print(forward_bic$call)

# Model Comparisons
cat("\nModels Summary Comparison:\n")
cat("AIC Backward:", AIC(backward_aic), "\n")
cat("AIC Forward:", AIC(forward_aic), "\n")
cat("BIC Backward:", BIC(backward_bic), "\n")
cat("BIC Forward:", BIC(forward_bic), "\n")
```

## Ridge Regression Model Optimization

```{r}
library(glmnet)
library(boot)

index <- sample(seq_len(nrow(data)), size = 2/3 * nrow(data))
trainData <- data[index, ]
testData <- data[-index, ]

X_train <- as.matrix(trainData[, -ncol(trainData)])
y_train <- trainData$LC50
X_test <- as.matrix(testData[, -ncol(testData)])
y_test <- testData$LC50

lambda_grid <- 10^seq(3, -2, lenght = 100)

# Ridge regression (CV)
cv_ridge <- cv.glmnet(X_train, y_train, alpha = 0,
                      lambda = lambda_grid, nfolds = 10)
optimal_lambda_cv <- cv_ridge$lambda.min

cat("Optimal lambda from Cross-Validation:", optimal_lambda_cv, "\n")

ridge_model <- glmnet(X_train, y_train, alpha = 0,
                      lambda = optimal_lambda_cv)
train_predictions_ridge <- predict(ridge_model, newx = X_train)
test_predictions_ridge <- predict(ridge_model, newx = X_test)
mse_train_ridge <- mean((y_train - train_predictions_ridge)^2)
mse_test_ridge <- mean((y_test - test_predictions_ridge)^2)
cat("Training Error (Ridge):", mse_train_ridge, "\n")
cat("Test Error (Ridge):", mse_test_ridge, "\n")
```

```{r}
# Ridge regression (bootstrap)
bootstrap_mse <- function(data, indices, lambda) {
  # Create bootstrap sample
  bootstrap_sample <- data[indices, ]
  X_bootstrap <- as.matrix(bootstrap_sample[, -ncol(bootstrap_sample)])
  y_bootstrap <- bootstrap_sample$LC50
  
  model <- glmnet(X_bootstrap, y_bootstrap, alpha = 0, lambda = lambda)
  y_pred <- predict(model, s = lambda, newx = X_test)
  mse <- mean((y_test - y_pred)^2)
  return(mse)
}
# Bootstrap for multiple lambda values
bootstrap_results <- sapply(lambda_grid, function(lambda) {
  mse_values <- replicate(100, boot(trainData, bootstrap_mse, R = 1,
                                    lambda = lambda)$t)
  return(mean(mse_values))
})
# Optimal Lambda from Bootstrap
optimal_lambda_bootstrap <- lambda_grid[which.min(bootstrap_results)]
cat("Optimal lambda from Bootstrap:", optimal_lambda_bootstrap, "\n")
```

```{r}
results_df <- data.frame(
  Lambda = lambda_grid,
  CV_MSE = sapply(lambda_grid,
                  function(l) mean(cv_ridge$cvm[cv_ridge$lambda == l])),
  Bootstrap_MSE = bootstrap_results
)

# Plot
ggplot(results_df, aes(x = log10(Lambda)) ) +
  geom_line(aes(y = CV_MSE, color = "Cross-Validation"), size = 1) +
  geom_line(aes(y = Bootstrap_MSE, color = "Bootstrap"), size = 1) +
  labs(title = "Comparison of MSE from Cross-Validation and Bootstrap",
       x = "Log10(Lambda)",
       y = "Mean Squared Error (MSE)",
       color = "Method") +
  theme_light() +
  theme(legend.position = "bottom",
        panel.grid = element_blank(),
        plot.title = element_text(hjust = 0.5),
        text = element_text(size = 10))
```

## Non-linear Modeling Using Generalized Additive Models

```{r}
sapply(trainData, function(x) length(unique(x)))
```

```{r}
library(mgcv)

set.seed(2024)
index <- sample(seq_len(nrow(data)), size = 2/3 * nrow(data))
trainData <- data[index, ]
testData <- data[-index, ]

X_train <- as.matrix(trainData[, -ncol(trainData)])
y_train <- trainData$LC50
X_test <- as.matrix(testData[, -ncol(testData)])
y_test <- testData$LC50

# GAM less complexity (k = -1)
k = 1
gam_model_1 <- gam(LC50 ~ s(TPSA, k=k) + s(SAacc, k=k) + s(H050, k=k) + 
                     s(MLOGP, k=k) + s(RDCHI, k=k) + s(GATS1p, k=k) +
                     s(nN, k=k) + s(C040, k=k), data = trainData)

pred_gam_train_1 <- predict(gam_model_1, newdata = trainData)
pred_gam_test_1 <- predict(gam_model_1, newdata = testData)
mse_train_gam_1 <- mean((y_train - pred_gam_train_1)^2)
mse_test_gam_1 <- mean((y_test - pred_gam_test_1)^2)

cat("Training Error (GAM - k=1):","\t",mse_train_gam_1, "\n")
cat("Test Error (GAM - k=1):","\t",mse_test_gam_1, "\n")

# GAM more complexity (k = 6)
k = 6
gam_model_2 <- gam(LC50 ~ s(TPSA, k=k) + s(SAacc, k=k) + s(H050, k=k) + 
                     s(MLOGP, k=k) + s(RDCHI, k=k) + s(GATS1p, k=k) +
                     s(nN, k=k) + s(C040, k=k), data = trainData)

pred_gam_train_2 <- predict(gam_model_2, newdata = trainData)
pred_gam_test_2 <- predict(gam_model_2, newdata = testData)
mse_train_gam_2 <- mean((y_train - pred_gam_train_2)^2)
mse_test_gam_2 <- mean((y_test - pred_gam_test_2)^2)

cat("Training Error (GAM - k=6):","\t",mse_train_gam_2, "\n")
cat("Test Error (GAM - k=6):","\t",mse_test_gam_2, "\n")
```

## Regression Tree Model with Cost-Complexity Pruning

```{r}
library(rpart)
library(rpart.plot)

tree_model <- rpart(LC50 ~ ., data = trainData, method = "anova")

train_predictions_tree1 <- predict(tree_model, newdata = trainData)
test_predictions_tree1 <- predict(tree_model, newdata = testData)

mse_train_tree1 <- mean((trainData$LC50 - train_predictions_tree1)^2)
mse_test_tree1 <- mean((testData$LC50 - test_predictions_tree1)^2)

printcp(tree_model)
cat("Training Error (Tree):", mse_train_tree1, "\n")
cat("Test Error (Tree):", mse_test_tree1, "\n")
```

```{r}
rpart.plot(tree_model, 
           main = "Customized Regression Tree for LC50",
           type = 3,
           fallen.leaves = TRUE,
           box.palette = "RdYlGn",
           cex = 0.6,
           tweak = 1,
           split.cex = 0.9,
           split.lty = 3,
)
```

```{r}
optimal_cp <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]), "CP"]

pruned_tree <- prune(tree_model, cp = optimal_cp)
train_predictions_tree <- predict(pruned_tree, newdata = trainData)
test_predictions_tree <- predict(pruned_tree, newdata = testData)

mse_train_tree <- mean((trainData$LC50 - train_predictions_tree)^2)
mse_test_tree <- mean((testData$LC50 - test_predictions_tree)^2)

rpart.plot(pruned_tree, 
           main = "Pruned Regression Tree for LC50",
           type = 3,
           fallen.leaves = TRUE,
           box.palette = "RdYlGn",
           cex = 0.6,
           tweak = 1,
           split.cex = 0.9,
           split.lty = 3,
)
```

```{r}
printcp(pruned_tree)
cat("Training Error (Tree):", mse_train_tree, "\n")
cat("Test Error (Tree):", mse_test_tree, "\n")
```

## Comparative Analysis of Regression Models

### Training and Test Error Assessment for Various Models

```{r}
library(tidyr)

model_comparison <- data.frame(
  Model = c("Linear", "Linear (dummy)", "Ridge", "GAM", "Tree"),
  Training_Error = c(error_train, error_train_dummy,
                     mse_train_ridge, mse_train_gam_2, mse_train_tree),
  Test_Error = c(error_test, error_test_dummy,
                 mse_test_ridge, mse_test_gam_2, mse_test_tree)
)

model_comparison_long <- pivot_longer(model_comparison, 
                                      cols = c("Training_Error", "Test_Error"), 
                                      names_to = "Error_Type", 
                                      values_to = "MSE")

# Plot
ggplot(model_comparison_long, aes(x = Model, y = MSE, fill = Error_Type)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
  labs(title = "Comparison of Training and Test Errors",
       x = "Model",
       y = "Mean Squared Error (MSE)",
       fill = "Error Type") +
  theme_light() +
  theme(legend.position = "bottom",
        panel.grid = element_blank(),
        plot.title = element_text(hjust = 0.5),
        text = element_text(size = 10))
```

```{r}
ggplot(model_comparison_long, aes(x = Model, y = MSE,
                                  color = Error_Type, group = Error_Type)) +
  geom_line(size = 0.9) +
  labs(title = "Line Plot Comparison of Training and Test Errors",
       x = "Model",
       y = "Mean Squared Error (MSE)",
       color = "Error Type") +
  theme_light() +
  theme(legend.position = "bottom",
        panel.grid = element_blank(),
        plot.title = element_text(hjust = 0.5),
        text = element_text(size = 10))
```

# Classification Model Comparison for Diabetes Prediction

## k-NN Classification and Cross-Validation Comparison

```{r}
library(mlbench)
data(PimaIndiansDiabetes2)
df <- PimaIndiansDiabetes2
head(df)
```

```{r}
na_count <- sapply(df, function(x) sum(is.na(x)))
print(na_count)
```

```{r}
# KNN imputation

library(mlbench)
library(VIM)

df_imp <- kNN(df, k = 5)
df_imp <- df_imp[, 1:9]
df_imp$diabetes <- as.factor(df_imp$diabetes)
head(df_imp)
```

```{r}
na_count_df_imp <- sapply(df_imp, function(x) sum(is.na(x)))
print(na_count_df_imp)
```

```{r}
library(caret)
set.seed(2024)
trainIndex <- createDataPartition(df_imp$diabetes, 
                                  p = 2/3, 
                                  list = FALSE, 
                                  times = 1)

trainData <- df_imp[trainIndex, ]
testData <- df_imp[-trainIndex, ]

trainData$diabetes <- ifelse(trainData$diabetes == "pos", 1, 0)
testData$diabetes <- ifelse(testData$diabetes == "pos", 1, 0)
trainData$diabetes <- as.factor(trainData$diabetes)
testData$diabetes <- as.factor(testData$diabetes)

head(trainData)
```

```{r}
library(class)
library(mlbench)
# Function: cross-validated k-NN errors
cv_knn_errors <- function(data, k_values, cv_type) {
    errors <- numeric(length(k_values))
    for (i in seq_along(k_values)) {
        k <- k_values[i]
        if (cv_type == "LOOCV") {
            control <- trainControl(method = "LOOCV")
        } else if (cv_type == "10-fold") {
            control <- trainControl(method = "cv", number = 10)
        }
        
        model <- train(diabetes ~ ., data = data, method = "knn",
                       trControl = control, tuneGrid = data.frame(k = k))
        errors[i] <- min(model$results$Accuracy)
    }
    return(1 - errors)
}

# k values (tested that >25 is too much)
k_values <- 1:25

# 5 fold
errors_10fold <- cv_knn_errors(df_imp, k_values, "10-fold")

# LOOCV
errors_loocv <- cv_knn_errors(df_imp, k_values, "LOOCV")

trainIndex <- createDataPartition(df_imp$diabetes, p = .67, list = FALSE)
trainData <- df_imp[trainIndex, ]
testData <- df_imp[-trainIndex, ]

# Loop for each k
test_errors <- numeric(length(k_values))
for (i in seq_along(k_values)) {
    k <- k_values[i]
    predictions <- knn(train = trainData[,-ncol(trainData)], 
                       test = testData[,-ncol(testData)],
                       cl = trainData$diabetes, k = k)
    test_errors[i] <- mean(predictions != testData$diabetes)
}
```

```{r}
# Create a data frame for plotting
error_data <- data.frame(
    k = k_values,
    LOOCV_Error = errors_loocv,
    Fold10_Error = errors_10fold,
    Test_Error = test_errors
)

error_data_long <- reshape2::melt(error_data, id.vars = "k")

# Plotting
ggplot(error_data_long, aes(x = k, y = value, color = variable)) +
    geom_line(size = 0.7) +
    # geom_point(size = 1.3) +
    labs(title = "k-NN Classifier Errors",
         x = "Number of Neighbors (k)",
         y = "Error Rate",
         color = "Error Type") +
    scale_x_continuous(breaks = k_values) +
    theme_light() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          legend.position = "bottom",
          panel.grid = element_blank(),
          plot.title = element_text(hjust = 0.5),
          text = element_text(size = 10))
```

## Generalized Additive Model for Diabetes Prediction

```{r}
library(caret)
library(mgcv)
library(dplyr)
library(broom)

# model
gam_model <- gam(diabetes ~ s(pregnant) + s(glucose) + s(pressure) + 
                 s(triceps) + s(insulin) + s(mass) + 
                 s(pedigree) + s(age), data = trainData, family = binomial)
summary(gam_model)
```

```{r}
# library(MASS)
# # full model
# full_model <- gam(diabetes ~ s(pregnant) + s(glucose) + s(pressure) + 
#                    s(triceps) + s(insulin) + s(mass) + 
#                    s(pedigree) + s(age), 
#                    data = trainData, family = binomial)
# # stepwise AIC
# selected_model <- step(full_model, direction = "backward")
# summary(selected_model)
```

## Tree-Based Methods for Diabetes Classification

```{r Classification Tree}
library(rpart)
library(ipred)
library(randomForest)
library(caret)

classification_tree <- rpart(diabetes ~ ., data = trainData, method = "class")

tree_train_pred <- predict(classification_tree, trainData, type = "class")
tree_test_pred <- predict(classification_tree, testData, type = "class")

# error
tree_train_error <- mean(tree_train_pred != trainData$diabetes)
tree_test_error <- mean(tree_test_pred != testData$diabetes)

cat("Classification Tree Training Error:", tree_train_error, "\n")
cat("Classification Tree Test Error:", tree_test_error, "\n")
```

```{r Ensemble of Bagged Tree}
bagged_trees <- bagging(diabetes ~ ., data = trainData)

bagged_train_pred <- predict(bagged_trees, trainData)
bagged_test_pred <- predict(bagged_trees, testData)

# error
bagged_train_error <- mean(bagged_train_pred != trainData$diabetes)
bagged_test_error <- mean(bagged_test_pred != testData$diabetes)

cat("Bagged Trees Training Error:", bagged_train_error, "\n")
cat("Bagged Trees Test Error:", bagged_test_error, "\n")
```

```{r Random Forest}
# Fit the random forest
random_forest <- randomForest(diabetes ~ ., data = trainData)

# Predict on training and test data
rf_train_pred <- predict(random_forest, trainData)
rf_test_pred <- predict(random_forest, testData)

# Calculate training and test error
rf_train_error <- mean(rf_train_pred != trainData$diabetes)
rf_test_error <- mean(rf_test_pred != testData$diabetes)

cat("Random Forest Training Error:", rf_train_error, "\n")
cat("Random Forest Test Error:", rf_test_error, "\n")
```

```{r Plot Comparison}
# Store the errors in a data frame
results <- data.frame(
  Model = c("Classification Tree", "Bagged Trees", "Random Forest"),
  Training_Error = c(tree_train_error, bagged_train_error, rf_train_error),
  Test_Error = c(tree_test_error, bagged_test_error, rf_test_error)
)

# Reshape the data for ggplot
results_long <- reshape2::melt(results, id.vars = "Model", variable.name = "Error_Type", value.name = "Error")

ggplot(results_long, aes(x = Model, y = Error, fill = Error_Type)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9),
           width = 0.7) +
  geom_text(aes(label = round(Error, 3)),
            position = position_dodge(width = 0.9),
            vjust = -0.5, size = 2) +  # Data labels
  theme_light() +
  labs(title = "Comparison of Training and Test Errors",
       x = "Model",
       y = "Error Rate",
       fill = "Error Type") +
  theme(legend.position = "bottom",
        panel.grid = element_blank(),
        plot.title = element_text(hjust = 0.5),
        axis.title = element_text(),
        text = element_text(size = 10))
```

## Neural Network Modeling for Diabetes Prediction

```{r}
library(nnet)
library(caret)

neurons <- seq(1, 100, by = 5)

# Initializing
train_errors <- c()
test_errors <- c()

# Loop
for (n in neurons) {
  # Fit NN
  nn_model <- nnet(diabetes ~ ., data = trainData,
                   size = n, linout = FALSE, maxit = 500, trace = FALSE)
  
  # Train
  train_pred <- predict(nn_model, trainData)
  train_pred_class <- ifelse(train_pred > 0.5, 1, 0)  # Convert to binary
  train_error <- mean(train_pred_class != trainData$diabetes)
  train_errors <- c(train_errors, train_error)
  
  # Test
  test_pred <- predict(nn_model, testData)
  test_pred_class <- ifelse(test_pred > 0.5, 1, 0)  # Convert to binary
  test_error <- mean(test_pred_class != testData$diabetes)
  test_errors <- c(test_errors, test_error)
}

# Results
error_results <- data.frame(
  Neurons = neurons,
  Training_Error = train_errors,
  Test_Error = test_errors
)
print(error_results)

# Choose the optimal number of neurons based on test error
optimal_neurons <- neurons[which.min(test_errors)]
cat("Optimal number of neurons:", optimal_neurons, "\n")

```

```{r}
library(ggplot2)
error_results_long <- reshape2::melt(error_results,
                                     id.vars = "Neurons", 
                                     variable.name = "Error_Type", 
                                     value.name = "Error")

ggplot(error_results_long, aes(x = Neurons, y = log10(Error),
                               color = Error_Type)) +
  geom_line(size = 0.7) +
  labs(title = "Training and Test Errors for Neural Network",
       x = "Number of Neurons",
       y = "log(Error Rate)") +
  theme_light() +
  theme(legend.position = "bottom",
        panel.grid = element_blank(),
        plot.title = element_text(hjust = 0.5),
        text = element_text(size = 10))
```

## Comparing Classification Methods for Diabetes Analysis

```{r}

```
